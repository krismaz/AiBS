\section{Experiments}

All data had been collected with the unix tool \verb|perf|, each
datapoint being the average over 10 runs. The machine that ran the
tests has the following specs:
\begin{description}
\item[CPU:] Intel i7 quadcore 2.67GHz
\item[RAM:] 6GB
\item[Operating System:] Ubuntu 14.04 LTS
\item[Python version:] 3.4.0
\end{description}

This first plot shows $N$ on the horizontal axis and the actual
(average) running time divided by the expected asymptotic complexity
($O(N^2)$) on the vertical axis. The plot shows that sub-quadratic
factors play an overwhelming role on datasets smaller than $N \approx
100$.
\begin{center}
	\includegraphics[width=\textwidth]{../plots/plot1.pdf}
\end{center}

In this second plot, we have zoomed in on roughly the lower half of
the plot from before. Both lines seem to approach horizontal as $N$
increases. This suggests that the asymptotic complexity of $O(N^2)$ is
right.
\begin{center}
	\includegraphics[width=\textwidth]{../plots/plot2.pdf}
\end{center}

The last plot shows the ratio between the execution times of the two
algorithms. It can be seen that they are roughly equally fast in the
beginning, but as $N$ increases towards 400, the linear algorithm
becomes comparatively faster than the affine algorithm. Around $N
\approx 400$ it seems to stabilize as $T(affine)/T(linear) \approx
1.7$, meaning that the affine algorithm takes approximately 70\%
longer than the corresponding linear case.
\begin{center}
	\includegraphics[width=\textwidth]{../plots/plot3.pdf}
\end{center}
